# -*- coding=utf-8 -*-
__author__ = "Group 1"

import gradio as gr
import requests
import os
import json
import time
import re
from typing import Dict, List, Tuple
import shutil
from PIL import Image
import io
import cv2
from pathlib import Path
import ffmpeg
import imageio_ffmpeg


class MultiGPU_ServiceClient:
    def __init__(self):
        self.servers = {
            "Local GPU": {"host": "127.0.0.1", "port": 8010},
            "Tesla T4": {"host": "127.0.0.1", "port": 8070},
            "AMD 7900": {"host": "140.124.181.195", "port": 8010}
        }

        # å‰µå»ºå·¥ä½œç›®éŒ„
        self.workspace_dir = "./gradio_workspace"
        self.upload_dir = os.path.join(self.workspace_dir, "uploads")
        self.download_dir = os.path.join(self.workspace_dir, "downloads")

        for dir_path in [self.workspace_dir, self.upload_dir, self.download_dir]:
            os.makedirs(dir_path, exist_ok=True)

    def get_base_url(self, server_name: str) -> str:
        server_info = self.servers[server_name]
        return f"http://{server_info['host']}:{server_info['port']}"

    def validate_and_process_image(self, image_path: str) -> str:
        """é©—è­‰ä¸¦è™•ç†åœ–ç‰‡"""
        try:
            # å˜—è©¦æ‰“é–‹åœ–ç‰‡æª¢æŸ¥æ˜¯å¦æœ‰æ•ˆ
            with Image.open(image_path) as img:
                # æª¢æŸ¥åœ–ç‰‡æ ¼å¼
                if img.format not in ['JPEG', 'JPG', 'PNG', 'BMP']:
                    # è½‰æ›ç‚ºJPEGæ ¼å¼
                    processed_path = image_path.replace(os.path.splitext(image_path)[1], '_processed.jpg')
                    img.convert('RGB').save(processed_path, 'JPEG')
                    return processed_path

                # æª¢æŸ¥åœ–ç‰‡å°ºå¯¸ï¼Œå¦‚æœå¤ªå¤§å°±ç¸®å°
                max_size = (1920, 1080)
                if img.size[0] > max_size[0] or img.size[1] > max_size[1]:
                    img.thumbnail(max_size, Image.Resampling.LANCZOS)
                    processed_path = image_path.replace(os.path.splitext(image_path)[1], '_resized.jpg')
                    img.save(processed_path, 'JPEG')
                    return processed_path

            return image_path
        except Exception as e:
            print(f"åœ–ç‰‡è™•ç†éŒ¯èª¤: {str(e)}")
            return None

    def upload_model(self, server_name: str, model_file_path: str) -> dict:
        try:
            if not os.path.exists(model_file_path):
                return {"error": f"æ¨¡å‹æª”æ¡ˆä¸å­˜åœ¨: {model_file_path}"}

            base_url = self.get_base_url(server_name)

            with open(model_file_path, "rb") as f:
                files = {"file": (os.path.basename(model_file_path), f)}
                response = requests.post(f"{base_url}/upload-model/", files=files, timeout=30)

            return response.json()
        except Exception as e:
            return {"error": f"ä¸Šå‚³æ¨¡å‹å¤±æ•—: {str(e)}"}

    def upload_img(self, server_name: str, img_file_path: str, model_name: str) -> dict:
        try:
            if not os.path.exists(img_file_path):
                return {"error": f"åœ–ç‰‡æª”æ¡ˆä¸å­˜åœ¨: {img_file_path}"}

            base_url = self.get_base_url(server_name)

            with open(img_file_path, "rb") as f:
                files = {"file": (os.path.basename(img_file_path), f)}
                response = requests.post(f"{base_url}/upload-img/{model_name}", files=files, timeout=60)

            return response.json()
        except Exception as e:
            return {"error": f"ä¸Šå‚³åœ–ç‰‡å¤±æ•—: {str(e)}"}

    def download_file(self, server_name: str, file_name: str, base_folder: str) -> str:
        try:
            base_url = self.get_base_url(server_name)
            response = requests.get(f"{base_url}/download-image/{file_name}/{base_folder}", stream=True, timeout=30)

            if response.status_code == 200:
                local_path = os.path.join(self.download_dir, base_folder, file_name)
                os.makedirs(os.path.dirname(local_path), exist_ok=True)

                with open(local_path, "wb") as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        f.write(chunk)

                return local_path
            else:
                return None
        except Exception as e:
            print(f"ä¸‹è¼‰æª”æ¡ˆéŒ¯èª¤: {str(e)}")
            return None


class YOLO_MultiGPU_UI:
    def __init__(self):
        self.client = MultiGPU_ServiceClient()

    def parse_time_info(self, time_content: str) -> str:
        """è§£ææ™‚é–“çµ±è¨ˆï¼Œæ ¼å¼åŒ–é¡¯ç¤º"""
        try:
            if not time_content:
                return "ç„¡æ™‚é–“è³‡è¨Š"

            # æå–ä¸‰å€‹ä¸»è¦æ™‚é–“æŒ‡æ¨™
            load_speed = re.search(r'load model speed:\s*([\d.]+)\s*ms', time_content)
            inference_speed = re.search(r'Inference speed:\s*([\d.]+)\s*ms', time_content)
            summary_speed = re.search(r'print summary speed:\s*([\d.]+)\s*ms', time_content)

            result = "â±ï¸ åŸ·è¡Œæ™‚é–“çµ±è¨ˆ\n" + "=" * 30 + "\n"

            if load_speed:
                result += f"ğŸ”„ æ¨¡å‹è¼‰å…¥æ™‚é–“: {float(load_speed.group(1)):.2f} ms\n"
            else:
                result += f"ğŸ”„ æ¨¡å‹è¼‰å…¥æ™‚é–“: N/A\n"

            if inference_speed:
                result += f"ğŸš€ æ¨ç†åŸ·è¡Œæ™‚é–“: {float(inference_speed.group(1)):.2f} ms\n"
            else:
                result += f"ğŸš€ æ¨ç†åŸ·è¡Œæ™‚é–“: N/A\n"

            if summary_speed:
                result += f"ğŸ“ çµæœè™•ç†æ™‚é–“: {float(summary_speed.group(1)):.2f} ms\n"
            else:
                result += f"ğŸ“ çµæœè™•ç†æ™‚é–“: N/A\n"

            # è¨ˆç®—ç¸½æ™‚é–“
            if load_speed and inference_speed and summary_speed:
                total_time = float(load_speed.group(1)) + float(inference_speed.group(1)) + float(
                    summary_speed.group(1))
                result += f"\nâš¡ ç¸½è™•ç†æ™‚é–“: {total_time:.2f} ms"

            return result

        except Exception as e:
            return f"æ™‚é–“è§£æéŒ¯èª¤: {str(e)}"

    def parse_object_statistics(self, result_json_content: str) -> str:
        """è§£æç‰©ä»¶çµ±è¨ˆï¼ŒåŒ…å«æ•¸é‡å’Œå¹³å‡ç½®ä¿¡åº¦"""
        try:
            if not result_json_content:
                return "ç„¡æª¢æ¸¬çµæœ"

            # æ¸…ç†è¼¸å…¥æ•¸æ“šï¼Œç§»é™¤é¦–å°¾ç©ºç™½å­—ç¬¦
            cleaned_content = result_json_content.strip()

            # å˜—è©¦è§£æJSONæ•¸æ“š
            detection_results = None

            # æ–¹æ³•1: å¦‚æœå·²ç¶“æ˜¯å­—å…¸æˆ–åˆ—è¡¨ï¼Œç›´æ¥ä½¿ç”¨
            if isinstance(cleaned_content, (dict, list)):
                detection_results = cleaned_content
            else:
                # æ–¹æ³•2: å˜—è©¦JSONè§£æ
                try:
                    detection_results = json.loads(cleaned_content)
                except json.JSONDecodeError as e:
                    # æ–¹æ³•3: å˜—è©¦ä½¿ç”¨evalï¼ˆåƒ…ç•¶å…§å®¹çœ‹èµ·ä¾†å®‰å…¨æ™‚ï¼‰
                    try:
                        # æª¢æŸ¥å…§å®¹æ˜¯å¦çœ‹èµ·ä¾†åƒPythonå­—å…¸/åˆ—è¡¨æ ¼å¼
                        if cleaned_content.startswith('[') and cleaned_content.endswith(']'):
                            detection_results = eval(cleaned_content)
                        else:
                            return f"JSONè§£æéŒ¯èª¤: {str(e)}\nåŸå§‹å…§å®¹å‰100å­—ç¬¦:\n{cleaned_content[:100]}"
                    except Exception as eval_error:
                        return f"æ•¸æ“šè§£æå¤±æ•—:\nJSONéŒ¯èª¤: {str(e)}\nEvaléŒ¯èª¤: {str(eval_error)}\nåŸå§‹å…§å®¹:\n{cleaned_content[:200]}"

            # é©—è­‰è§£æçµæœ
            if detection_results is None:
                return "è§£æçµæœç‚ºç©º"

            if not isinstance(detection_results, list):
                return f"æœŸæœ›åˆ—è¡¨æ ¼å¼ï¼Œä½†ç²å¾—: {type(detection_results)}"

            if not detection_results:
                return "æœªæª¢æ¸¬åˆ°ä»»ä½•ç‰©ä»¶"

            # çµ±è¨ˆæ¯å€‹é¡åˆ¥çš„ç‰©ä»¶
            class_stats = {}

            for i, detection in enumerate(detection_results):
                try:
                    # ç¢ºä¿detectionæ˜¯å­—å…¸
                    if not isinstance(detection, dict):
                        continue

                    class_name = detection.get('name', 'unknown')
                    confidence = detection.get('confidence', 0.0)

                    # ç¢ºä¿confidenceæ˜¯æ•¸å­—
                    if not isinstance(confidence, (int, float)):
                        try:
                            confidence = float(confidence)
                        except (ValueError, TypeError):
                            confidence = 0.0

                    if class_name not in class_stats:
                        class_stats[class_name] = {
                            'count': 0,
                            'confidences': []
                        }

                    class_stats[class_name]['count'] += 1
                    class_stats[class_name]['confidences'].append(confidence)

                except Exception as item_error:
                    print(f"è™•ç†ç¬¬{i}å€‹æª¢æ¸¬é …ç›®æ™‚å‡ºéŒ¯: {str(item_error)}")
                    continue

            if not class_stats:
                return "æ²’æœ‰æœ‰æ•ˆçš„æª¢æ¸¬çµæœ"

            # æ ¼å¼åŒ–è¼¸å‡º
            result = "ğŸ“Š ç‰©ä»¶æª¢æ¸¬çµ±è¨ˆ\n" + "=" * 30 + "\n"
            result += f"ğŸ¯ ç¸½æª¢æ¸¬ç‰©ä»¶æ•¸: {len(detection_results)}\n"
            result += f"ğŸ·ï¸  æª¢æ¸¬é¡åˆ¥æ•¸: {len(class_stats)}\n\n"

            result += "ğŸ“ˆ å„é¡åˆ¥è©³ç´°çµ±è¨ˆ:\n" + "-" * 25 + "\n"

            # æŒ‰æª¢æ¸¬æ•¸é‡æ’åº
            sorted_classes = sorted(class_stats.items(), key=lambda x: x[1]['count'], reverse=True)

            for class_name, stats in sorted_classes:
                count = stats['count']
                confidences = stats['confidences']

                if confidences:
                    avg_confidence = sum(confidences) / len(confidences)
                    max_confidence = max(confidences)
                    min_confidence = min(confidences)
                else:
                    avg_confidence = max_confidence = min_confidence = 0.0

                result += f"ğŸ”¸ {class_name}:\n"
                result += f"   æ•¸é‡: {count}\n"
                result += f"   å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.3f}\n"
                result += f"   æœ€é«˜ç½®ä¿¡åº¦: {max_confidence:.3f}\n"
                result += f"   æœ€ä½ç½®ä¿¡åº¦: {min_confidence:.3f}\n\n"

            return result

        except Exception as e:
            return f"ç‰©ä»¶çµ±è¨ˆè§£æéŒ¯èª¤: {str(e)}\nè«‹æª¢æŸ¥æ•¸æ“šæ ¼å¼æ˜¯å¦æ­£ç¢º"

    def process_inference(self, server_name: str, image_file, model_file) -> Tuple[str, str, str, str]:
        """åŸ·è¡Œæ¨ç†éç¨‹"""
        try:
            if not server_name:
                return None, "âŒ è«‹é¸æ“‡GPUæœå‹™å™¨", "", ""

            if not image_file or not model_file:
                return None, "âŒ è«‹ä¸Šå‚³åœ–ç‰‡å’Œæ¨¡å‹æª”æ¡ˆ", "", ""

            # ä¿å­˜ä¸Šå‚³çš„æª”æ¡ˆåˆ°æœ¬åœ°
            timestamp = time.strftime("%Y%m%d_%H%M%S")

            log_info = f"ğŸš€ é–‹å§‹è™•ç†...\nä½¿ç”¨æœå‹™å™¨: {server_name}\n"

            # è™•ç†å’Œé©—è­‰åœ–ç‰‡
            log_info += "ğŸ–¼ï¸  é©—è­‰åœ–ç‰‡æ ¼å¼...\n"
            processed_image_path = self.client.validate_and_process_image(image_file)
            if not processed_image_path:
                return None, "âŒ åœ–ç‰‡æ ¼å¼ç„¡æ•ˆæˆ–è™•ç†å¤±æ•—", "", ""

            # ä¿å­˜åœ–ç‰‡
            img_filename = f"input_image_{timestamp}.jpg"
            img_path = os.path.join(self.client.upload_dir, img_filename)
            shutil.copy2(processed_image_path if processed_image_path != image_file else image_file, img_path)

            # ä¿å­˜æ¨¡å‹
            model_filename = f"model_{timestamp}.pt"
            model_path = os.path.join(self.client.upload_dir, model_filename)
            shutil.copy2(model_file, model_path)

            log_info += "âœ… åœ–ç‰‡é©—è­‰æˆåŠŸ\n"

            # 1. ä¸Šå‚³æ¨¡å‹
            log_info += "ğŸ“¤ ä¸Šå‚³æ¨¡å‹ä¸­...\n"
            model_result = self.client.upload_model(server_name, model_path)
            if "error" in model_result:
                return None, f"âŒ æ¨¡å‹ä¸Šå‚³å¤±æ•—: {model_result['error']}", "", ""

            log_info += "âœ… æ¨¡å‹ä¸Šå‚³æˆåŠŸ\n"

            # 2. ä¸Šå‚³åœ–ç‰‡ä¸¦åŸ·è¡Œæ¨ç†
            log_info += "ğŸ“¤ ä¸Šå‚³åœ–ç‰‡ä¸¦åŸ·è¡Œæ¨ç†...\n"
            img_result = self.client.upload_img(server_name, img_path, os.path.basename(model_path))
            if "error" in img_result:
                return None, f"âŒ åœ–ç‰‡è™•ç†å¤±æ•—: {img_result['error']}", "", ""

            log_info += "âœ… æ¨ç†å®Œæˆ\n"

            # 3. ä¸‹è¼‰çµæœ
            base_folder = img_result.get('baseF', '')
            if not base_folder:
                return None, "âŒ ç„¡æ³•ç²å–çµæœè³‡æ–™å¤¾ä¿¡æ¯", "", ""

            log_info += "ğŸ“¥ ä¸‹è¼‰çµæœä¸­...\n"

            # ä¸‹è¼‰è™•ç†éçš„åœ–ç‰‡
            processed_image_path = None
            if 'processed_images' in img_result and img_result['processed_images']:
                for img_path in img_result['processed_images']:
                    downloaded_path = self.client.download_file(
                        server_name, os.path.basename(img_path), base_folder
                    )
                    if downloaded_path and os.path.exists(downloaded_path):
                        processed_image_path = downloaded_path
                        break

            # ä¸‹è¼‰æ™‚é–“è³‡è¨Š
            formatted_time_info = ""
            if 'time_path' in img_result:
                time_file_path = self.client.download_file(
                    server_name, os.path.basename(img_result['time_path']), base_folder
                )
                if time_file_path and os.path.exists(time_file_path):
                    with open(time_file_path, 'r', encoding='utf-8') as f:
                        time_content = f.read()
                        formatted_time_info = self.parse_time_info(time_content)

            # ä¸‹è¼‰ä¸¦è§£ææª¢æ¸¬çµæœ
            formatted_object_stats = ""
            if 'summary_file_path' in img_result:
                summary_file_path = self.client.download_file(
                    server_name, os.path.basename(img_result['summary_file_path']), base_folder
                )
                if summary_file_path and os.path.exists(summary_file_path):
                    with open(summary_file_path, 'r', encoding='utf-8') as f:
                        summary_content = f.read()

                    # èª¿è©¦ä¿¡æ¯ï¼šæ‰“å°åŸå§‹å…§å®¹
                    print(f"åŸå§‹summaryå…§å®¹: {summary_content[:200]}...")
                    print(f"å…§å®¹é¡å‹: {type(summary_content)}")
                    print(f"å…§å®¹é•·åº¦: {len(summary_content)}")

                    # è§£æçµ±è¨ˆä¿¡æ¯
                    formatted_object_stats = self.parse_object_statistics(summary_content)

            # æ ¼å¼åŒ–æœ€çµ‚çµæœ
            final_log = f"""
ğŸ¯ æ¨ç†å®Œæˆï¼

ğŸ“Š æœå‹™å™¨: {server_name}
ğŸ“ çµæœè³‡æ–™å¤¾: {base_folder}

âœ¨ è™•ç†ç‹€æ…‹: æ‰€æœ‰æ­¥é©ŸæˆåŠŸå®Œæˆ
ğŸ“ˆ æª¢æ¸¬çµæœå·²ç”Ÿæˆ
            """.strip()

            return processed_image_path, final_log, formatted_time_info, formatted_object_stats

        except Exception as e:
            return None, f"âŒ è™•ç†éç¨‹å‡ºéŒ¯: {str(e)}", "", ""

    def process_inference_vid(self, server_name: str, video_file, model_file) -> Tuple[str, str, str, str]:
        """åŸ·è¡Œå½±ç‰‡æ¨ç†æµç¨‹ï¼ˆåˆ†å¹€+é‡çµ„ + è‡ªå‹•è½‰æª”ï¼‰"""
        try:
            if not server_name:
                return None, "âŒ è«‹é¸æ“‡GPUæœå‹™å™¨", "", ""

            if not video_file or not model_file:
                return None, "âŒ è«‹ä¸Šå‚³å½±ç‰‡å’Œæ¨¡å‹æª”æ¡ˆ", "", ""

            # æª”æ¡ˆè™•ç†èˆ‡å‘½å
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            log_info = f"ğŸ¬ é–‹å§‹è™•ç†å½±ç‰‡...\nä½¿ç”¨æœå‹™å™¨: {server_name}\n"

            video_filename = f"input_video_{timestamp}.mp4"
            video_path = os.path.join(self.client.upload_dir, video_filename)
            shutil.copy2(video_file, video_path)

            model_filename = f"model_{timestamp}.pt"
            model_path = os.path.join(self.client.upload_dir, model_filename)
            shutil.copy2(model_file, model_path)

            # åˆ†å‰²å½±ç‰‡ç‚º frames
            log_info += "ğŸ” åˆ†å‰²å½±ç‰‡ç‚ºå½±æ ¼...\n"
            base_name = Path(video_path).stem
            frames_dir = os.path.join(self.client.upload_dir, f"frames_{base_name}")
            os.makedirs(frames_dir, exist_ok=True)

            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                return None, "âŒ ç„¡æ³•è®€å–å½±ç‰‡", "", ""

            fps = cap.get(cv2.CAP_PROP_FPS)
            frame_count = 0
            success, frame = cap.read()
            while success:
                frame_path = os.path.join(frames_dir, f"frame_{frame_count:04d}.jpg")
                cv2.imwrite(frame_path, frame)
                frame_count += 1
                success, frame = cap.read()
            cap.release()

            log_info += f"âœ… æˆåŠŸæ“·å– {frame_count} å¼µå½±æ ¼\n"

            # ä¸Šå‚³æ¨¡å‹
            log_info += "ğŸ“¤ ä¸Šå‚³æ¨¡å‹...\n"
            model_result = self.client.upload_model(server_name, model_path)
            if "error" in model_result:
                return None, f"âŒ æ¨¡å‹ä¸Šå‚³å¤±æ•—: {model_result['error']}", "", ""
            log_info += "âœ… æ¨¡å‹ä¸Šå‚³æˆåŠŸ\n"

            # ä¸Šå‚³æ‰€æœ‰ frames ä¸¦æ¨ç†
            log_info += "ğŸ§  é–‹å§‹å½±æ ¼æ¨ç†...\n"
            processed_frame_paths = []
            for fname in sorted(os.listdir(frames_dir)):
                if not fname.endswith(".jpg"):
                    continue
                frame_path = os.path.join(frames_dir, fname)
                img_result = self.client.upload_img(server_name, frame_path, os.path.basename(model_path))
                if "error" in img_result:
                    return None, f"âŒ å½±æ ¼æ¨ç†å¤±æ•—: {img_result['error']}", "", ""

                base_folder = img_result.get("baseF", "")
                processed_files = img_result.get("processed_images", [])
                if processed_files:
                    downloaded_path = self.client.download_file(server_name, os.path.basename(processed_files[0]), base_folder)
                    if downloaded_path:
                        processed_frame_paths.append(downloaded_path)

            # é‡çµ„å½±ç‰‡
            log_info += "ğŸï¸ é‡çµ„æ¨ç†å¾Œå½±ç‰‡...\n"
            if not processed_frame_paths:
                return None, "âŒ ç„¡æ¨ç†å¾Œå½±æ ¼å¯ç”¨", "", ""

            sample_img = cv2.imread(processed_frame_paths[0])
            height, width = sample_img.shape[:2]
            raw_video_path = os.path.join(self.client.upload_dir, f"{base_name}_raw_result.mp4")
            out = cv2.VideoWriter(raw_video_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))

            for img_path in processed_frame_paths:
                frame = cv2.imread(img_path)
                out.write(frame)
            out.release()

            log_info += f"âœ… æˆåŠŸç”Ÿæˆå½±ç‰‡ï¼Œå…± {len(processed_frame_paths)} å¹€\n"

            # ğŸ”„ è½‰æª”ç‚ºç€è¦½å™¨ç›¸å®¹å½±ç‰‡æ ¼å¼ (H.264)
            result_video_path = os.path.join(self.client.upload_dir, f"{base_name}_result_fixed.mp4")
            ffmpeg_bin = imageio_ffmpeg.get_ffmpeg_exe()  # è‡ªå‹•å–å¾—å…§å»º ffmpeg
            try:
                (
                    ffmpeg
                    .input(raw_video_path)
                    .output(
                        result_video_path,
                        vcodec='libx264',
                        acodec='aac',
                        movflags='+faststart'
                    )
                    .run(cmd=ffmpeg_bin, overwrite_output=True, capture_stdout=True, capture_stderr=True)
                )
                log_info += f"âœ… å·²è½‰æª”ç‚ºç€è¦½å™¨ç›¸å®¹æ ¼å¼\n"
            except Exception as e:
                return None, f"âŒ ffmpeg è½‰æª”éç¨‹éŒ¯èª¤: {str(e)}", "", ""

            # è™•ç†æ™‚é–“èˆ‡çµ±è¨ˆè³‡è¨Š
            formatted_time_info = ""
            formatted_object_stats = ""
            if 'time_path' in img_result:
                time_path = self.client.download_file(server_name, os.path.basename(img_result['time_path']), base_folder)
                if os.path.exists(time_path):
                    with open(time_path, 'r') as f:
                        formatted_time_info = self.parse_time_info(f.read())

            if 'summary_file_path' in img_result:
                summary_path = self.client.download_file(server_name, os.path.basename(img_result['summary_file_path']), base_folder)
                if os.path.exists(summary_path):
                    with open(summary_path, 'r') as f:
                        summary_content = f.read()
                        formatted_object_stats = self.parse_object_statistics(summary_content)

            final_log = f"""
    ğŸ¬ æ¨ç†å®Œæˆï¼å½±ç‰‡é‡çµ„æˆåŠŸ âœ…

    ğŸ–¥ï¸ ä½¿ç”¨ä¼ºæœå™¨: {server_name}
    ğŸ“¹ è¼¸å‡ºå½±ç‰‡: {result_video_path}
    ğŸ“ˆ æ¨ç†å½±æ ¼ç¸½æ•¸: {len(processed_frame_paths)}
    """.strip()

            return result_video_path, final_log, formatted_time_info, formatted_object_stats

        except Exception as e:
            return None, f"âŒ è™•ç†éç¨‹ç™¼ç”ŸéŒ¯èª¤: {str(e)}", "", ""
        
    def build_interface(self):
        with gr.Blocks(
                theme=gr.themes.Soft(),
                css="""
            .server-selection {
                background: linear-gradient(45deg, #667eea 0%, #764ba2 100%);
                border-radius: 10px;
                padding: 20px;
                margin: 10px 0;
            }
            .result-container {
                background: #f8f9fa;
                border-radius: 10px;
                padding: 15px;
                margin: 10px 0;
                border-left: 4px solid #28a745;
            }
            .stats-container {
                background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                color: white;
                border-radius: 10px;
                padding: 15px;
                margin: 10px 0;
            }
            .time-container {
                background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                color: white;
                border-radius: 10px;
                padding: 15px;
                margin: 10px 0;
                border-left: 4px solid #2196f3;
            }
            """
        ) as demo:
            gr.Markdown(
                """
                # ğŸš€ å¤šGPU YOLOç‰©ä»¶æª¢æ¸¬ç³»çµ±
                ### æ”¯æ´ RTX 3050Tiã€Tesla T4ã€AMD 7900 ä¸‰ç¨®åŠ é€Ÿæ–¹æ¡ˆ
                """,
                elem_classes=["server-selection"]
            )
            with gr.Tabs():
                with gr.Tab("åœ–ç‰‡"):
                    with gr.Row():
                        # å·¦å´æ§åˆ¶é¢æ¿
                        with gr.Column(scale=1):
                            gr.Markdown("### ğŸ›ï¸ æ§åˆ¶é¢æ¿")

                            server_dropdown = gr.Dropdown(
                                choices=["Local GPU", "Tesla T4", "AMD 7900"],
                                label="ğŸ–¥ï¸ é¸æ“‡GPUæœå‹™å™¨",
                                value="Local GPU"
                            )

                            image_upload = gr.File(
                                label="ğŸ“· ä¸Šå‚³åœ–ç‰‡",
                                file_types=[".jpg", ".jpeg", ".png", ".bmp"],
                                type="filepath"
                            )

                            model_upload = gr.File(
                                label="ğŸ¤– ä¸Šå‚³YOLOæ¨¡å‹",
                                file_types=[".pt"],
                                type="filepath"
                            )

                            process_btn = gr.Button(
                                "ğŸš€ é–‹å§‹æ¨ç†",
                                variant="primary",
                                size="lg"
                            )
                        
                        # å³å´çµæœé¡¯ç¤º
                        with gr.Column(scale=2):
                            gr.Markdown("### ğŸ“Š æ¨ç†çµæœ")

                            with gr.Tabs():
                                with gr.Tab("ğŸ–¼ï¸ æª¢æ¸¬çµæœ"):
                                    result_image = gr.Image(
                                        label="æª¢æ¸¬çµæœåœ–ç‰‡",
                                        type="filepath"
                                    )

                                with gr.Tab("ğŸ“ è©³ç´°æ—¥èªŒ"):
                                    log_output = gr.Textbox(
                                        label="è™•ç†æ—¥èªŒ",
                                        lines=15,
                                        max_lines=20,
                                        elem_classes=["result-container"]
                                    )

                    # åº•éƒ¨çµ±è¨ˆä¿¡æ¯
                    with gr.Row():
                        with gr.Column():
                            gr.Markdown("### â±ï¸ æ™‚é–“çµ±è¨ˆ", elem_classes=["time-container"])
                            time_stats = gr.Textbox(
                                label="åŸ·è¡Œæ™‚é–“è©³æƒ…",
                                lines=8,
                                interactive=False
                            )

                        with gr.Column():
                            gr.Markdown("### ğŸ“ˆ ç‰©ä»¶çµ±è¨ˆ", elem_classes=["stats-container"])
                            object_stats = gr.Textbox(
                                label="æª¢æ¸¬ç‰©ä»¶è©³æƒ…",
                                lines=8,
                                interactive=False
                            )

                    
                    # äº‹ä»¶ç¶å®š
                    process_btn.click(
                        fn=self.process_inference,
                        inputs=[server_dropdown, image_upload, model_upload],
                        outputs=[result_image, log_output, time_stats, object_stats]
                    )

                    # ç¤ºä¾‹èªªæ˜
                    gr.Markdown(
                        """
                        ---
                        ### ğŸ“‹ ä½¿ç”¨èªªæ˜ï¼š
                        1. **é¸æ“‡GPUæœå‹™å™¨**ï¼šæ ¹æ“šæ‚¨çš„éœ€æ±‚é¸æ“‡åˆé©çš„åŠ é€Ÿç¡¬é«”
                        2. **ä¸Šå‚³æª”æ¡ˆ**ï¼šé¸æ“‡è¦æª¢æ¸¬çš„åœ–ç‰‡å’ŒYOLOæ¨¡å‹æª”æ¡ˆ
                        3. **é–‹å§‹æ¨ç†**ï¼šé»æ“ŠæŒ‰éˆ•é–‹å§‹ç‰©ä»¶æª¢æ¸¬
                        4. **æŸ¥çœ‹çµæœ**ï¼šåœ¨å³å´æŸ¥çœ‹æª¢æ¸¬çµæœåœ–ç‰‡å’Œè©³ç´°çµ±è¨ˆ

                        ### ğŸ’¡ æœå‹™å™¨è¦æ ¼ï¼š
                        - **RTX 3050Ti**: æœ¬æ©ŸWindowsç’°å¢ƒï¼Œé©åˆå¿«é€Ÿæ¸¬è©¦
                        - **Tesla T4**: é›²ç«¯Linuxç’°å¢ƒï¼Œå¹³è¡¡æ€§èƒ½èˆ‡æˆæœ¬
                        - **AMD 7900**: é«˜æ€§èƒ½Linuxç’°å¢ƒï¼Œé©åˆå¤§å‹ä»»å‹™

                        ### ğŸ”§ æ”¹å–„é …ç›®ï¼š
                        - **åœ–ç‰‡è™•ç†**: è‡ªå‹•é©—è­‰ä¸¦è™•ç†ä¸åŒæ ¼å¼çš„åœ–ç‰‡
                        - **æ™‚é–“çµ±è¨ˆ**: æ¸…æ™°é¡¯ç¤ºæ¨¡å‹è¼‰å…¥ã€æ¨ç†ã€è™•ç†ä¸‰å€‹éšæ®µæ™‚é–“
                        - **ç‰©ä»¶çµ±è¨ˆ**: æŒ‰é¡åˆ¥é¡¯ç¤ºæ•¸é‡ã€å¹³å‡ç½®ä¿¡åº¦ç­‰è©³ç´°ä¿¡æ¯
                        """
                    )
                with gr.Tab("å½±ç‰‡"):
                    with gr.Row():
                        # å·¦å´æ§åˆ¶é¢æ¿
                        with gr.Column(scale=1):
                            gr.Markdown("### ğŸ›ï¸ æ§åˆ¶é¢æ¿")

                            server_dropdown = gr.Dropdown(
                                choices=["Local GPU", "Tesla T4", "AMD 7900"],
                                label="ğŸ–¥ï¸ é¸æ“‡GPUæœå‹™å™¨",
                                value="Local GPU"
                            )

                            video_upload = gr.File(
                                label="ğŸï¸ ä¸Šå‚³å½±ç‰‡",
                                file_types=[".mp4"],
                                type="filepath"
                            )

                            model_upload = gr.File(
                                label="ğŸ¤– ä¸Šå‚³YOLOæ¨¡å‹",
                                file_types=[".pt"],
                                type="filepath"
                            )

                            process_btn = gr.Button(
                                "ğŸš€ é–‹å§‹æ¨ç†",
                                variant="primary",
                                size="lg"
                            )
                        
                        # å³å´çµæœé¡¯ç¤º
                        with gr.Column(scale=2):
                            gr.Markdown("### ğŸ“Š æ¨ç†çµæœ")

                            with gr.Tabs():
                                with gr.Tab("ğŸ¬ æª¢æ¸¬å½±ç‰‡"):
                                    result_video = gr.Video(
                                        label="æª¢æ¸¬çµæœå½±ç‰‡",
                                        format="mp4",
                                        interactive=False
                                    )

                                with gr.Tab("ğŸ“ è©³ç´°æ—¥èªŒ"):
                                    log_output = gr.Textbox(
                                        label="è™•ç†æ—¥èªŒ",
                                        lines=15,
                                        max_lines=20,
                                        elem_classes=["result-container"]
                                    )

                    # åº•éƒ¨çµ±è¨ˆä¿¡æ¯
                    with gr.Row():
                        with gr.Column():
                            gr.Markdown("### â±ï¸ æ™‚é–“çµ±è¨ˆ", elem_classes=["time-container"])
                            time_stats = gr.Textbox(
                                label="åŸ·è¡Œæ™‚é–“è©³æƒ…",
                                lines=8,
                                interactive=False
                            )

                        with gr.Column():
                            gr.Markdown("### ğŸ“ˆ ç‰©ä»¶çµ±è¨ˆ", elem_classes=["stats-container"])
                            object_stats = gr.Textbox(
                                label="æª¢æ¸¬ç‰©ä»¶è©³æƒ…",
                                lines=8,
                                interactive=False
                            )

                    # äº‹ä»¶ç¶å®š
                    process_btn.click(
                        fn=self.process_inference_vid,
                        inputs=[server_dropdown, video_upload, model_upload],
                        outputs=[result_video, log_output, time_stats, object_stats]
                    )

                    # ä½¿ç”¨èªªæ˜
                    gr.Markdown(
                        """
                        ---
                        ### ğŸ“‹ ä½¿ç”¨èªªæ˜ï¼š
                        1. **é¸æ“‡GPUæœå‹™å™¨**ï¼šæ ¹æ“šæ‚¨çš„éœ€æ±‚é¸æ“‡åˆé©çš„åŠ é€Ÿç¡¬é«”
                        2. **ä¸Šå‚³æª”æ¡ˆ**ï¼šé¸æ“‡è¦æª¢æ¸¬çš„å½±ç‰‡å’ŒYOLOæ¨¡å‹æª”æ¡ˆ
                        3. **é–‹å§‹æ¨ç†**ï¼šé»æ“ŠæŒ‰éˆ•é–‹å§‹ç‰©ä»¶æª¢æ¸¬
                        4. **æŸ¥çœ‹çµæœ**ï¼šåœ¨å³å´æŸ¥çœ‹æ¨ç†å¾Œå½±ç‰‡å’Œè©³ç´°çµ±è¨ˆ

                        ### ğŸ’¡ æœå‹™å™¨è¦æ ¼ï¼š
                        - **RTX 3050Ti**: æœ¬æ©ŸWindowsç’°å¢ƒï¼Œé©åˆå¿«é€Ÿæ¸¬è©¦
                        - **Tesla T4**: é›²ç«¯Linuxç’°å¢ƒï¼Œå¹³è¡¡æ€§èƒ½èˆ‡æˆæœ¬
                        - **AMD 7900**: é«˜æ€§èƒ½Linuxç’°å¢ƒï¼Œé©åˆå¤§å‹ä»»å‹™

                        ### ğŸ”§ æ”¹å–„é …ç›®ï¼š
                        - **å½±ç‰‡è™•ç†**: è‡ªå‹•åˆ†å¹€èˆ‡åˆä½µè¼¸å‡º
                        - **æ™‚é–“çµ±è¨ˆ**: æ¸…æ™°é¡¯ç¤ºæ¨¡å‹è¼‰å…¥ã€æ¨ç†ã€è™•ç†ä¸‰å€‹éšæ®µæ™‚é–“
                        - **ç‰©ä»¶çµ±è¨ˆ**: æŒ‰é¡åˆ¥é¡¯ç¤ºæ•¸é‡ã€å¹³å‡ç½®ä¿¡åº¦ç­‰è©³ç´°ä¿¡æ¯
                        """
                    )

                    return demo


def main():
    ui = YOLO_MultiGPU_UI()
    interface = ui.build_interface()

    try:
        interface.queue().launch(
            allowed_paths=[
                "./gradio_workspace",
                os.path.abspath("./gradio_workspace")
            ],
            server_name="127.0.0.1",
            server_port=7860,
            share=False
        )
    except Exception as e:
        print(f"å•Ÿå‹•ä¼ºæœå™¨æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
    finally:
        print("ä¼ºæœå™¨é—œé–‰")


if __name__ == '__main__':
    main()